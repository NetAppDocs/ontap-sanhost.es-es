---
sidebar: sidebar 
permalink: nvme_esxi_8.html 
keywords: nvme, esxi, ontap, nvme/fc, hypervisor 
summary: 'Es posible configurar NVMe over Fabrics (NVMe-oF) en hosts del iniciador que ejecutan ESXi 8.x y ONTAP como destino.' 
---
= Configuración de host de NVMe-oF para ESXi 8.x con ONTAP
:hardbreaks:
:toclevels: 1
:allow-uri-read: 
:toclevels: 1
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./media/


[role="lead"]
Es posible configurar NVMe over Fabrics (NVMe-oF) en hosts del iniciador que ejecutan ESXi 8.x y ONTAP como destino.



== Compatibilidad

* A partir de ONTAP 9.16,1 la asignación de espacio está habilitada de forma predeterminada en todos los espacios de nombres NVMe recién creados.
* A partir de ONTAP 9.9.1 P3, el protocolo NVMe/FC es compatible con ESXi 8 y versiones posteriores.
* A partir de la versión 9.10.1 de ONTAP, el protocolo NVMe/TCP es compatible con ONTAP.




== Funciones

* Los hosts de iniciadores ESXi pueden ejecutar tráfico NVMe/FC y FCP a través de los mismos puertos de adaptador. Consulte link:https://hwu.netapp.com/Home/Index["Hardware Universe"^] Para obtener una lista de controladoras y adaptadores de FC admitidos. Consulte link:https://mysupport.netapp.com/matrix/["Herramienta de matriz de interoperabilidad de NetApp"^] para obtener la lista más actual de configuraciones y versiones compatibles.
* Para ESXi 8,0 y versiones posteriores, HPP (complemento de alto rendimiento) es el complemento predeterminado para los dispositivos NVMe.




== Limitaciones conocidas

* No se admite la asignación de RDM.




== Habilite NVMe/FC

NVMe/FC está habilitado de forma predeterminada en las versiones de vSphere.

.Verifique el NQN del host
Debe comprobar la cadena NQN del host ESXi y comprobar que coincida con la cadena NQN del host del subsistema correspondiente en la cabina de ONTAP.

[listing]
----
# esxcli nvme info get
----
Resultado de ejemplo:

[listing]
----
Host NQN: nqn.2014-08.org.nvmexpress:uuid:62a19711-ba8c-475d-c954-0000c9f1a436
----
[listing]
----
# vserver nvme subsystem host show -vserver nvme_fc
----
Resultado de ejemplo:

[listing]
----
Vserver Subsystem Host NQN
------- --------- ----------------------------------------------------------
nvme_fc nvme_ss  nqn.2014-08.org.nvmexpress:uuid:62a19711-ba8c-475d-c954-0000c9f1a436
----
Si las cadenas del host NQN no coinciden, se debe usar `vserver nvme subsystem host add` Comando para actualizar la cadena NQN de host correcta en el subsistema NVMe de ONTAP correspondiente.



== Configuración de Broadcom/Emulex y Marvell/Qlogic

La `lpfc` conductor y el `qlnativefc` El controlador en vSphere 8.x tiene la funcionalidad NVMe/FC habilitada de forma predeterminada.

Consulte link:https://mysupport.netapp.com/matrix/["Herramienta de matriz de interoperabilidad"^] para comprobar si la configuración es compatible con el controlador o el firmware.



== Valide NVMe/FC

Es posible usar el siguiente procedimiento para validar NVMe/FC.

.Pasos
. Compruebe que el adaptador NVMe/FC esté en la lista en el host ESXi:
+
[listing]
----
# esxcli nvme adapter list
----
+
Resultado de ejemplo:

+
[listing]
----

Adapter  Adapter Qualified Name           Transport Type  Driver      Associated Devices
-------  -------------------------------  --------------  ----------  ------------------
vmhba64  aqn:lpfc:100000109b579f11        FC              lpfc
vmhba65  aqn:lpfc:100000109b579f12        FC              lpfc
vmhba66  aqn:qlnativefc:2100f4e9d456e286  FC              qlnativefc
vmhba67  aqn:qlnativefc:2100f4e9d456e287  FC              qlnativefc
----
. Compruebe que los espacios de nombres NVMe/FC se han creado correctamente:
+
Los UUID en el siguiente ejemplo representan los dispositivos de espacio de nombres NVMe/FC.

+
[listing, subs="+quotes"]
----
# esxcfg-mpath -b
uuid.116cb7ed9e574a0faf35ac2ec115969d : NVMe Fibre Channel Disk (*uuid.116cb7ed9e574a0faf35ac2ec115969d*)
   vmhba64:C0:T0:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:50 WWPN: 21:00:00:24:ff:7f:4a:50  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:05:d0:39:ea:3a:b2:1f
   vmhba64:C0:T1:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:50 WWPN: 21:00:00:24:ff:7f:4a:50  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:07:d0:39:ea:3a:b2:1f
   vmhba65:C0:T1:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:51 WWPN: 21:00:00:24:ff:7f:4a:51  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:08:d0:39:ea:3a:b2:1f
   vmhba65:C0:T0:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:51 WWPN: 21:00:00:24:ff:7f:4a:51  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:06:d0:39:ea:3a:b2:1f
----
+
[NOTE]
====
En ONTAP 9,7, el tamaño de bloque predeterminado para un espacio de nombres NVMe/FC es de 4K KB. El tamaño predeterminado no es compatible con ESXi. Por lo tanto, al crear espacios de nombres para ESXi, debe establecer el tamaño del bloque de espacio de nombres como *512B*. Puede hacer esto mediante el `vserver nvme namespace create` comando.

Ejemplo:

`vserver nvme namespace create -vserver vs_1 -path /vol/nsvol/namespace1 -size 100g -ostype vmware -block-size 512B`

Consulte la link:https://docs.netapp.com/us-en/ontap/concepts/manual-pages.html["Páginas manuales de comandos de ONTAP 9"^] para obtener más detalles.

====
. Compruebe el estado de las rutas ANA individuales de los dispositivos de espacio de nombres NVMe/FC respectivos:
+
[listing, subs="+quotes"]
----
# esxcli storage hpp path list -d uuid.df960bebb5a74a3eaaa1ae55e6b3411d

fc.20000024ff7f4a50:21000024ff7f4a50-fc.2004d039ea3ab21f:2005d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba64:C0:T0:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=ANO*,health=UP}

fc.20000024ff7f4a51:21000024ff7f4a51-fc.2004d039ea3ab21f:2008d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba65:C0:T1:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=AO*,health=UP}

fc.20000024ff7f4a51:21000024ff7f4a51-fc.2004d039ea3ab21f:2006d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba65:C0:T0:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=ANO*,health=UP}

fc.20000024ff7f4a50:21000024ff7f4a50-fc.2004d039ea3ab21f:2007d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba64:C0:T1:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=AO*,health=UP}

----




== Configure NVMe/TCP

En ESXi 8.x, los módulos NVMe/TCP necesarios se cargan de forma predeterminada. Para configurar la red y el adaptador NVMe/TCP, consulte la documentación de VMware vSphere.



== Valide NVMe/TCP

Puede usar el siguiente procedimiento para validar NVMe/TCP.

.Pasos
. Compruebe el estado del adaptador NVMe/TCP:
+
[listing]
----
esxcli nvme adapter list
----
+
Resultado de ejemplo:

+
[listing]
----
Adapter  Adapter Qualified Name           Transport Type  Driver   Associated Devices
-------  -------------------------------  --------------  -------  ------------------
vmhba65  aqn:nvmetcp:ec-2a-72-0f-e2-30-T  TCP             nvmetcp  vmnic0
vmhba66  aqn:nvmetcp:34-80-0d-30-d1-a0-T  TCP             nvmetcp  vmnic2
vmhba67  aqn:nvmetcp:34-80-0d-30-d1-a1-T  TCP             nvmetcp  vmnic3
----
. Recupere una lista de conexiones NVMe/TCP:
+
[listing]
----
esxcli nvme controller list
----
+
Resultado de ejemplo:

+
[listing]
----
Name                                                  Controller Number  Adapter  Transport Type  Is Online  Is VVOL
---------------------------------------------------------------------------------------------------------  -----------------  -------
nqn.2014-08.org.nvmexpress.discovery#vmhba64#192.168.100.166:8009  256  vmhba64  TCP                  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.165:4420 258  vmhba64  TCP  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.168:4420 259  vmhba64  TCP  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.166:4420 260  vmhba64  TCP  true    false
nqn.2014-08.org.nvmexpress.discovery#vmhba64#192.168.100.165:8009  261  vmhba64  TCP                  true    false
nqn.2014-08.org.nvmexpress.discovery#vmhba65#192.168.100.155:8009  262  vmhba65  TCP                  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.167:4420 264  vmhba64  TCP  true    false

----
. Recupere una lista del número de rutas a un espacio de nombres NVMe:
+
[listing, subs="+quotes"]
----
esxcli storage hpp path list -d *uuid.f4f14337c3ad4a639edf0e21de8b88bf*
----
+
Resultado de ejemplo:

+
[listing, subs="+quotes"]
----
tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.165:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T0:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=AO*,health=UP}

tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.168:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T3:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=ANO*,health=UP}

tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.166:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T2:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=ANO*,health=UP}

tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.167:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T1:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=AO*,health=UP}
----




== Active la asignación de espacio

La asignación de espacio es compatible con ESXi 8.x y versiones posteriores.

Cuando se habilita la asignación de espacio, si un espacio de nombres se queda sin espacio, ONTAP se comunica al host que no hay espacio libre disponible para las operaciones de escritura; el espacio de nombres sigue en línea y las operaciones de lectura siguen prestando servicio. Las operaciones de escritura se reanudan cuando se dispone del espacio libre adicional. La asignación de espacio también permite al host realizar `UNMAP` (a veces denominadas `TRIM`) operaciones. Las OPERACIONES DE ANULACIÓN DE ASIGNACIÓN permiten que un host identifique bloques de datos que ya no son necesarios porque ya no contienen datos válidos. A continuación, el sistema de almacenamiento puede desasignar esos bloques de datos de modo que el espacio se pueda consumir en otro lugar.

.Antes de empezar
link:https://docs.netapp.com/us-en/ontap/san-admin/enable-space-allocation.html["Asigne espacio en su sistema de almacenamiento de ONTAP"^]. A continuación, debe realizar los siguientes pasos en el host ESXi.

.Pasos
. En el host ESXi, compruebe que DSM está deshabilitado:
+
`esxcfg-advcfg -g /SCSi/NVmeUseDsmTp4040`

+
El valor esperado es 0.

. Habilite el DSM NVMe:
+
`esxcfg-advcfg -s 1 /Scsi/NvmeUseDsmTp4040`

. Compruebe que DSM está activado:
+
`esxcfg-advcfg -g /SCSi/NVmeUseDsmTp4040`

+
El valor esperado es 1.





== Problemas conocidos

La configuración de host de NVMe-oF para ESXi 8.x con ONTAP tiene los siguientes problemas conocidos:

[cols="10,30,30"]
|===
| ID de error de NetApp | Título | Descripción 


| link:https://mysupport.netapp.com/site/bugs-online/product/ONTAP/BURT/1420654["1420654"^] | Nodo de ONTAP no operativo cuando se utiliza el protocolo NVMe/FC con la versión 9.9.1 de ONTAP | ONTAP 9.9.1 ha introducido compatibilidad con el comando «abort» de NVMe. Cuando ONTAP recibe el comando «abort» para anular un comando NVMe fusionado que está esperando su comando de partner, se produce una interrupción en el nodo de ONTAP. El problema solo se observa con los hosts que usan comandos fusionados de NVMe (por ejemplo, ESX) y transporte de Fibre Channel (FC). 


| 1543660 | Se produce un error de I/O cuando las máquinas virtuales de Linux que utilizan adaptadores vNVMe encuentran una ventana larga Todas las rutas inactivas (APD)  a| 
Las máquinas virtuales de Linux que ejecutan vSphere 8.x y versiones posteriores, y que utilizan adaptadores NVMe virtuales (vNVME) encuentran un error de I/O porque la operación de reintento de vNVMe está deshabilitada de forma predeterminada. Para evitar una interrupción en las VM de Linux que ejecutan kernels antiguos durante una parada de todas las rutas (APD) o una carga de I/O pesada, VMware ha introducido un «VSCSIDisableNvmeRetry» ajustable para deshabilitar la operación de reintento de vNVMe.

|===
.Información relacionada
link:https://docs.netapp.com/us-en/ontap-apps-dbs/vmware/vmware-vsphere-overview.html["VMware vSphere con ONTAP"^] link:https://kb.vmware.com/s/article/2031038["Compatibilidad de VMware vSphere 5.x, 6.x y 7.x con MetroCluster de NetApp (2031038)"^] link:https://kb.vmware.com/s/article/83370["Compatibilidad de VMware vSphere 6.x y 7.x con la sincronización activa de SnapMirror de NetApp"^]
