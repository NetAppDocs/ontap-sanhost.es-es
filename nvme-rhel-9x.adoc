---
sidebar: sidebar 
permalink: nvme-rhel-9x.html 
keywords: nvme, linux, rhel, red hat, enterprise 
summary: Cómo configurar el host NVMe-oF para RHEL 9.x con ONTAP 
---
= Configurar RHEL 9.x para NVMe-oF con almacenamiento ONTAP
:hardbreaks:
:toclevels: 1
:allow-uri-read: 
:toclevels: 1
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./media/


[role="lead"]
Los hosts Red Hat Enterprise Linux (RHEL) admiten los protocolos NVMe sobre canal de fibra (NVMe/FC) y NVMe sobre TCP (NVMe/TCP) con acceso asimétrico al espacio de nombres (ANA).  ANA proporciona una funcionalidad de múltiples rutas equivalente al acceso a unidad lógica asimétrica (ALUA) en entornos iSCSI y FCP.

Aprenda a configurar hosts NVMe over Fabrics (NVMe-oF) para RHEL 9.x. Para obtener más información sobre asistencia y funciones, consulte link:nvme-rhel-supported-features.html["Compatibilidad y características de RHEL ONTAP"].

NVMe-oF con RHEL 9.x tiene las siguientes limitaciones conocidas:

* El `nvme disconnect-all` El comando desconecta los sistemas de archivos raíz y de datos y podría provocar inestabilidad del sistema.  No emita esto en sistemas que arrancan desde SAN a través de espacios de nombres NVMe-TCP o NVMe-FC.




== Paso 1: Opcionalmente, habilite el arranque SAN

Puede configurar su host para utilizar el arranque SAN para simplificar la implementación y mejorar la escalabilidad. Utilice ellink:https://mysupport.netapp.com/matrix/#welcome["Herramienta de matriz de interoperabilidad"^] para verificar que su sistema operativo Linux, el adaptador de bus de host (HBA), el firmware del HBA, el BIOS de arranque del HBA y la versión de ONTAP admitan el arranque SAN.

.Pasos
. https://docs.netapp.com/us-en/ontap/san-admin/create-nvme-namespace-subsystem-task.html["Cree un espacio de nombres NVMe y asígnelo al host"^] .
. Habilite el arranque SAN en el BIOS del servidor para los puertos a los que está asignado el espacio de nombres de arranque SAN.
+
Para obtener información acerca de cómo activar el BIOS HBA, consulte la documentación específica de su proveedor.

. Reinicie el host y verifique que el sistema operativo esté funcionando.




== Paso 2: Instale el software RHEL y NVMe y verifique su configuración

Para configurar su host para NVMe-oF, debe instalar los paquetes de software de host y NVMe, habilitar rutas múltiples y verificar la configuración NQN de su host.

.Pasos
. Instalar RHEL 9.x en el servidor. Una vez completada la instalación, verifique que esté ejecutando el kernel RHEL 9.x requerido:
+
[source, cli]
----
uname -r
----
+
Ejemplo de versión del kernel RHEL:

+
[listing]
----
5.14.0-611.5.1.el9_7.x86_64
----
. Instale el `nvme-cli` paquete:
+
[source, cli]
----
rpm -qa|grep nvme-cli
----
+
El siguiente ejemplo muestra un  `nvme-cli` versión del paquete:

+
[listing]
----
nvme-cli-2.13-1.el9.x86_64
----
. Instale el `libnvme` paquete:
+
[source, cli]
----
rpm -qa|grep libnvme
----
+
El siguiente ejemplo muestra un  `libnvme` versión del paquete:

+
[listing]
----
libnvme-1.13-1.el9.x86_64
----
. En el host, verifique la cadena hostnqn en  `/etc/nvme/hostnqn` :
+
[source, cli]
----
cat /etc/nvme/hostnqn
----
+
El siguiente ejemplo muestra un  `hostnqn` versión:

+
[listing]
----
nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0056-5410-8048-b9c04f425633
----
. En el sistema ONTAP , verifique que `hostnqn` La cadena coincide con la `hostnqn` cadena para el subsistema correspondiente en el sistema de almacenamiento ONTAP :
+
[source, cli]
----
::> vserver nvme subsystem host show -vserver vs_188
----
+
.Muestra el ejemplo
[%collapsible]
====
[listing]
----
Vserver Subsystem Priority  Host NQN
------- --------- --------  ------------------------------------------------
vs_188  Nvme1
                  regular   nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0056-5410-8048-b9c04f425633
        Nvme10
                  regular   nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0056-5410-8048-b9c04f425633
        Nvme11
                  regular   nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0056-5410-8048-b9c04f425633
        Nvme12
                  regular   nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0056-5410-8048-b9c04f425633
48 entries were displayed.
----
====



NOTE: Si el `hostnqn` Las cadenas no coinciden, utilice el `vserver modify` comando para actualizar el `hostnqn` cadena en el subsistema del sistema de almacenamiento ONTAP correspondiente para que coincida con el `hostnqn` cadena de `/etc/nvme/hostnqn` en el host.



== Paso 3: Configurar NVMe/FC y NVMe/TCP

Configure NVMe/FC con adaptadores Broadcom/Emulex o Marvell/QLogic, o configure NVMe/TCP mediante operaciones de descubrimiento y conexión manuales.

[role="tabbed-block"]
====
.NVMe/FC - Broadcom/Emulex
--
Configuración de NVMe/FC para un adaptador Broadcom/Emulex.

. Compruebe que está utilizando el modelo de adaptador admitido:
+
.. Mostrar los nombres de los modelos:
+
[source, cli]
----
cat /sys/class/scsi_host/host*/modelname
----
+
Debe ver la siguiente salida:

+
[listing]
----
LPe36002-M64
LPe36002-M64
----
.. Mostrar las descripciones del modelo:
+
[source, cli]
----
cat /sys/class/scsi_host/host*/modeldesc
----
+
Debería ver un resultado similar al siguiente ejemplo:

+
[listing]
----
Emulex LightPulse LPe36002-M64 2-Port 64Gb Fibre Channel Adapter
Emulex LightPulse LPe36002-M64 2-Port 64Gb Fibre Channel Adapter
----


. Compruebe que está utilizando la Broadcom recomendada `lpfc` firmware y controlador de bandeja de entrada:
+
.. Mostrar la versión del firmware:
+
[source, cli]
----
cat /sys/class/scsi_host/host*/fwrev
----
+
El comando devuelve las versiones del firmware:

+
[listing]
----
14.4.393.53, sli-4:6:d
14.4.393.53, sli-4:6:d
----
.. Mostrar la versión del controlador de la bandeja de entrada:
+
[source, cli]
----
cat /sys/module/lpfc/version
----
+
El siguiente ejemplo muestra una versión del controlador:

+
[listing]
----
0:14.4.0.9
----


+
Para obtener la lista actual de versiones de firmware y controladores de adaptador compatibles, consulte la link:https://mysupport.netapp.com/matrix/["Herramienta de matriz de interoperabilidad"^].

. Compruebe que `lpfc_enable_fc4_type` se establece en `3`:
+
[source, cli]
----
cat /sys/module/lpfc/parameters/lpfc_enable_fc4_type
----
. Compruebe que puede ver los puertos de iniciador:
+
[source, cli]
----
cat /sys/class/fc_host/host*/port_name
----
+
El siguiente ejemplo muestra las identidades del puerto:

+
[listing]
----
0x100000109bf044b1
0x100000109bf044b2
----
. Compruebe que los puertos de iniciador estén en línea:
+
[source, cli]
----
cat /sys/class/fc_host/host*/port_state
----
+
Debe ver la siguiente salida:

+
[listing]
----
Online
Online
----
. Compruebe que los puertos de iniciador NVMe/FC estén habilitados y que los puertos de destino estén visibles:
+
[source, cli]
----
cat /sys/class/scsi_host/host*/nvme_info
----
+
.Muestra el ejemplo
[%collapsible]
=====
[listing, subs="+quotes"]
----
NVME Initiator Enabled
XRI Dist lpfc0 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc0 WWPN x100000109b954518 WWNN x200000109b954518 DID x020700 *ONLINE*
NVME RPORT       WWPN x2022d039eaa7dfc8 WWNN x201fd039eaa7dfc8 DID x020b03 *TARGET DISCSRVC ONLINE*
NVME RPORT       WWPN x2023d039eaa7dfc8 WWNN x201fd039eaa7dfc8 DID x020103 *TARGET DISCSRVC ONLINE*

NVME Statistics
LS: Xmt 0000000548 Cmpl 0000000548 Abort 00000000
LS XMIT: Err 00000000  CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 0000000000001a68 Issue 0000000000001a68 OutIO 0000000000000000
        abort 00000000 noxri 00000000 nondlp 00000000 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 00000000 Err 00000000

NVME Initiator Enabled
XRI Dist lpfc1 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc1 WWPN x100000109b954519 WWNN x200000109b954519 DID x020500 *ONLINE*
NVME RPORT       WWPN x2027d039eaa7dfc8 WWNN x2025d039eaa7dfc8 DID x020b01 *TARGET DISCSRVC ONLINE*

NVME Statistics
LS: Xmt 00000005ab Cmpl 00000005ab Abort 00000000
LS XMIT: Err 00000000  CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 0000000000086ce1 Issue 0000000000086ce2 OutIO 0000000000000001
        abort 0000009c noxri 00000000 nondlp 00000002 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 000000b8 Err 000000b8

NVME Initiator Enabled
XRI Dist lpfc2 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc2 WWPN x100000109bf044b1 WWNN x200000109bf044b1 DID x022a00 *ONLINE*
NVME RPORT       WWPN x2027d039eaa7dfc8 WWNN x2025d039eaa7dfc8 DID x020b01 *TARGET DISCSRVC ONLINE*
NVME RPORT       WWPN x2011d039eaa7dfc8 WWNN x200fd039eaa7dfc8 DID x020b02 *TARGET DISCSRVC ONLINE*
NVME RPORT       WWPN x2002d039eaa7dfc8 WWNN x2000d039eaa7dfc8 DID x020b05 *TARGET DISCSRVC ONLINE*
NVME RPORT       WWPN x2026d039eaa7dfc8 WWNN x2025d039eaa7dfc8 DID x021301 *TARGET DISCSRVC ONLINE*
NVME RPORT       WWPN x2010d039eaa7dfc8 WWNN x200fd039eaa7dfc8 DID x021302 *TARGET DISCSRVC ONLINE*
NVME RPORT       WWPN x2001d039eaa7dfc8 WWNN x2000d039eaa7dfc8 DID x021305 *TARGET DISCSRVC ONLINE*

NVME Statistics
LS: Xmt 000000c186 Cmpl 000000c186 Abort 00000000
LS XMIT: Err 00000000  CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 00000000c348ca37 Issue 00000000c3344057 OutIO ffffffffffeb7620
        abort 0000815b noxri 000018b5 nondlp 00000116 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 0000915b Err 000c6091

NVME Initiator Enabled
XRI Dist lpfc3 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc3 WWPN x100000109bf044b2 WWNN x200000109bf044b2 DID x021b00 *ONLINE*
NVME RPORT       WWPN x2028d039eaa7dfc8 WWNN x2025d039eaa7dfc8 DID x020101 *TARGET DISCSRVC ONLINE*
NVME RPORT       WWPN x2012d039eaa7dfc8 WWNN x200fd039eaa7dfc8 DID x020102 *TARGET DISCSRVC ONLINE*
NVME RPORT       WWPN x2003d039eaa7dfc8 WWNN x2000d039eaa7dfc8 DID x020105 *TARGET DISCSRVC ONLINE*
NVME RPORT       WWPN x2029d039eaa7dfc8 WWNN x2025d039eaa7dfc8 DID x022901 *TARGET DISCSRVC ONLINE*
NVME RPORT       WWPN x2013d039eaa7dfc8 WWNN x200fd039eaa7dfc8 DID x022902 *TARGET DISCSRVC ONLINE*
NVME RPORT       WWPN x2004d039eaa7dfc8 WWNN x2000d039eaa7dfc8 DID x022905 *TARGET DISCSRVC ONLINE*

NVME Statistics
LS: Xmt 000000c186 Cmpl 000000c186 Abort 00000000
LS XMIT: Err 00000000  CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 00000000b5761af5 Issue 00000000b564b55e OutIO ffffffffffee9a69
        abort 000083d7 noxri 000016ea nondlp 00000195 qdepth 00000000 wqerr 00000002 err 00000000
FCP CMPL: xb 000094a4 Err 000c22e7
----
=====


--
.NVMe/FC - Marvell/QLogic
--
Configure NVMe/FC para un adaptador Marvell/QLogic.

. Verifique que esté utilizando el controlador del adaptador y las versiones de firmware compatibles:
+
[source, cli]
----
cat /sys/class/fc_host/host*/symbolic_name
----
+
El siguiente ejemplo muestra las versiones del controlador y del firmware:

+
[listing]
----
QLE2872 FW:v9.15.06 DVR:v10.02.09.400-k
QLE2872 FW:v9.15.06 DVR:v10.02.09.400-k
----
. Compruebe que `ql2xnvmeenable` está configurado. Esto permite que el adaptador Marvell funcione como iniciador NVMe/FC:
+
[source, cli]
----
cat /sys/module/qla2xxx/parameters/ql2xnvmeenable
----
+
La salida esperada es 1.



--
.NVMe/TCP
--
El protocolo NVMe/TCP no admite la operación de conexión automática.  En su lugar, puede descubrir los subsistemas y espacios de nombres NVMe/TCP realizando la prueba NVMe/TCP. `connect` o `connect-all` operaciones manualmente.

. Verifique que el puerto iniciador pueda obtener los datos de la página de registro de descubrimiento a través de los LIF NVMe/TCP compatibles:
+
[source, cli]
----
nvme discover -t tcp -w host-traddr -a traddr
----
+
.Muestra el ejemplo
[%collapsible]
=====
[listing, subs="+quotes"]
----
nvme discover -t tcp -w 192.168.30.15 -a 192.168.30.48

Discovery Log Number of Records 8, Generation counter 18
=====Discovery Log Entry 0======
trtype:  tcp
adrfam:  ipv4
subtype: *current discovery subsystem*
treq:    not specified
portid:  8
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.51a3c9846e0c11f08f5dd039eaa7dfc9:discovery
traddr:  192.168.31.49
eflags:  *explicit discovery connections, duplicate discovery information*
sectype: none
=====Discovery Log Entry 1======
trtype:  tcp
adrfam:  ipv4
subtype: *current discovery subsystem*
treq:    not specified
portid:  7
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.51a3c9846e0c11f08f5dd039eaa7dfc9:discovery
traddr:  192.168.31.48
eflags:  *explicit discovery connections, duplicate discovery information*
sectype: none
=====Discovery Log Entry 2======
trtype:  tcp
adrfam:  ipv4
subtype: *current discovery subsystem*
treq:    not specified
portid:  6
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.51a3c9846e0c11f08f5dd039eaa7dfc9:discovery
traddr:  192.168.30.49
eflags:  *explicit discovery connections, duplicate discovery information*
sectype: none
=====Discovery Log Entry 3======
trtype:  tcp
adrfam:  ipv4
subtype: *current discovery subsystem*
treq:    not specified
portid:  5
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.51a3c9846e0c11f08f5dd039eaa7dfc9:discovery
traddr:  192.168.30.48
eflags:  *explicit discovery connections, duplicate discovery information*
sectype: none
=====Discovery Log Entry 4======
trtype:  tcp
adrfam:  ipv4
subtype: *nvme subsystem*
treq:    not specified
portid:  8
trsvcid: 4420
subnqn:  nqn.1992-08.com.netapp:sn.51a3c9846e0c11f08f5dd039eaa7dfc9:subsystem.Nvme38
traddr:  192.168.31.49
eflags:  none
sectype: none
=====Discovery Log Entry 5======
trtype:  tcp
adrfam:  ipv4
subtype: *nvme subsystem*
treq:    not specified
portid:  7
trsvcid: 4420
subnqn:  nqn.1992-08.com.netapp:sn.51a3c9846e0c11f08f5dd039eaa7dfc9:subsystem.Nvme38
traddr:  192.168.31.48
eflags:  none
sectype: none
=====Discovery Log Entry 6======
trtype:  tcp
adrfam:  ipv4
subtype: *nvme subsystem*
treq:    not specified
portid:  6
trsvcid: 4420
subnqn:  nqn.1992-08.com.netapp:sn.51a3c9846e0c11f08f5dd039eaa7dfc9:subsystem.Nvme38
traddr:  192.168.30.49
eflags:  none
sectype: none
=====Discovery Log Entry 7======
trtype:  tcp
adrfam:  ipv4
subtype: *nvme subsystem*
treq:    not specified
portid:  5
trsvcid: 4420
subnqn:  nqn.1992-08.com.netapp:sn.51a3c9846e0c11f08f5dd039eaa7dfc9:subsystem.Nvme38
traddr:  192.168.30.48
eflags:  none
sectype: none
----
=====
. Verifique que las otras combinaciones de LIF de iniciador-destino NVMe/TCP puedan recuperar correctamente los datos de la página del registro de descubrimiento:
+
[source, cli]
----
nvme discover -t tcp -w host-traddr -a traddr
----
+
.Muestra el ejemplo
[%collapsible]
=====
[listing, subs="+quotes"]
----
nvme discover -t tcp -w 192.168.30.15 -a 192.168.30.48
nvme discover -t tcp -w 192.168.30.15 -a 192.168.30.49
nvme discover -t tcp -w 192.168.31.15 -a 192.168.31.48
nvme discover -t tcp -w 192.168.31.15 -a 192.168.31.49
----
=====
. Ejecute el `nvme connect-all` Comando en todos los LIF objetivo iniciador NVMe/TCP admitidos entre los nodos:
+
[source, cli]
----
nvme connect-all -t tcp -w host-traddr -a traddr
----
+
.Muestra el ejemplo
[%collapsible]
=====
[listing, subs="+quotes"]
----
nvme  connect-all -t  tcp -w  192.168.30.15 -a	192.168.30.48
nvme	connect-all	-t	tcp	-w	192.168.30.15	-a	192.168.30.49
nvme	connect-all	-t	tcp	-w	192.168.31.15	-a	192.168.31.48
nvme	connect-all	-t	tcp	-w	192.168.31.15	-a	192.168.31.49
----
=====


[NOTE]
====
A partir de RHEL 9.4, la configuración para NVMe/TCP  `ctrl_loss_tmo timeout` se establece automáticamente en "apagado". Como resultado:

* No hay límites en el número de reintentos (reintento indefinido).
* No es necesario configurar manualmente un elemento específico.  `ctrl_loss_tmo timeout` Duración al utilizar el  `nvme connect` o  `nvme connect-all` comandos (opción -l ).
* Los controladores NVMe/TCP no experimentan tiempos de espera en caso de una falla de ruta y permanecen conectados indefinidamente.


====
--
====


== Paso 4: Opcionalmente, modifique la iopolicy en las reglas de udev

RHEL 9.6 establece la política de io predeterminada para NVMe-oF en `round-robin` .  Si está utilizando RHEL 9.6 y desea cambiar la iopolicy a `queue-depth` , modifique el archivo de reglas de udev de la siguiente manera:

.Pasos
. Abra el archivo de reglas de udev en un editor de texto con privilegios de root:
+
[source, cli]
----
/usr/lib/udev/rules.d/71-nvmf-netapp.rules
----
+
Debe ver la siguiente salida:

+
[listing]
----
vi /usr/lib/udev/rules.d/71-nvmf-netapp.rules
----
. Busque la línea que establece iopolicy para el controlador NetApp ONTAP , como se muestra en la siguiente regla de ejemplo:
+
[listing]
----
ACTION=="add", SUBSYSTEM=="nvme-subsystem", ATTR{subsystype}=="nvm", ATTR{model}=="NetApp ONTAP Controller", ATTR{iopolicy}="round-robin"
----
. Modificar la regla para que `round-robin` se convierte `queue-depth` :
+
[source, cli]
----
ACTION=="add", SUBSYSTEM=="nvme-subsystem", ATTR{subsystype}=="nvm", ATTR{model}=="NetApp ONTAP Controller", ATTR{iopolicy}="queue-depth"
----
. Recargue las reglas udev y aplique los cambios:
+
[source, cli]
----
udevadm control --reload
udevadm trigger --subsystem-match=nvme-subsystem
----
. Verifique la iopolicy actual para su subsistema.  Reemplace <subsistema>, por ejemplo, `nvme-subsys0` .
+
[source, cli]
----
cat /sys/class/nvme-subsystem/<subsystem>/iopolicy
----
+
Debe ver la siguiente salida:

+
[listing]
----
queue-depth.
----



NOTE: La nueva iopolicy se aplica automáticamente a los dispositivos NetApp ONTAP Controller coincidentes. No es necesario reiniciar.



== Paso 5: Opcionalmente, habilite 1 MB de E/S para NVMe/FC

ONTAP informa un tamaño máximo de transferencia de datos (MDTS) de 8 en los datos del controlador de identificación.  Esto significa que el tamaño máximo de solicitud de E/S puede ser de hasta 1 MB.  Para emitir solicitudes de E/S de tamaño 1 MB para un host Broadcom NVMe/FC, debe aumentar el `lpfc` valor de la `lpfc_sg_seg_cnt` parámetro a 256 desde el valor predeterminado de 64.


NOTE: Estos pasos no se aplican a los hosts Qlogic NVMe/FC.

.Pasos
. Defina el `lpfc_sg_seg_cnt` parámetro en 256:
+
[source, cli]
----
cat /etc/modprobe.d/lpfc.conf
----
+
Debería ver un resultado similar al siguiente ejemplo:

+
[listing]
----
options lpfc lpfc_sg_seg_cnt=256
----
. Ejecute `dracut -f` el comando y reinicie el host.
. Compruebe que el valor de `lpfc_sg_seg_cnt` es 256:
+
[source, cli]
----
cat /sys/module/lpfc/parameters/lpfc_sg_seg_cnt
----




== Paso 6: Verificar los servicios de arranque NVMe

El `nvmefc-boot-connections.service` y `nvmf-autoconnect.service` Servicios de arranque incluidos en NVMe/FC `nvme-cli` Los paquetes se habilitan automáticamente cuando se inicia el sistema.

Una vez finalizado el arranque, verifique que  `nvmefc-boot-connections.service` y  `nvmf-autoconnect.service` Los servicios de arranque están habilitados.

.Pasos
. Compruebe que `nvmf-autoconnect.service` está activado:
+
[source, cli]
----
systemctl status nvmf-autoconnect.service
----
+
.Muestra el resultado de ejemplo
[%collapsible]
====
[listing]
----
nvmf-autoconnect.service - Connect NVMe-oF subsystems automatically during boot
        Loaded: loaded (/usr/lib/systemd/system/nvmf-autoconnect.service;  enabled; preset: disabled)

Active: inactive (dead) since Wed 2025-10-29 00:42:03 EDT; 6h ago   Main PID: 8487 (code=exited, status=0/SUCCESS)  CPU: 66ms

Oct 29 00:42:03 R650-14-188 systemd[1]: Starting Connect NVMe-oF subsystems automatically during boot...
Oct 29 00:42:03 R650-14-188 systemd[1]: nvmf-autoconnect.service: Deactivated successfully.
Oct 29 00:42:03 R650-14-188 systemd[1]: Finished Connect NVMe-oF subsystems automatically during boot.
----
====
. Compruebe que `nvmefc-boot-connections.service` está activado:
+
[source, cli]
----
systemctl status nvmefc-boot-connections.service
----
+
.Muestra el resultado de ejemplo
[%collapsible]
====
[listing]
----
nvmefc-boot-connections.service - Auto-connect to subsystems on FC-NVME devices found during boot
         Loaded: loaded (/usr/lib/systemd/system/nvmefc-boot-connections.service; enabled; preset:enabled)
     Active: inactive (dead) since Wed 2025-10-29 00:41:51 EDT; 6h ago
Main PID: 4652 (code=exited, status=0/SUCCESS)
        CPU: 13ms

Oct 29 00:41:51 R650-14-188 systemd[1]: Starting Auto-connect to subsystems on FC-NVME devices found during boot...  Oct 29 00:41:51 R650-14-188 systemd[1]: nvmefc-boot-connections.service: Deactivated successfully.  Oct 29 00:41:51 R650-14-188 systemd[1]: Finished Auto-connect to subsystems on FC-NVME devices found during boot
----
====




== Paso 7: Verificar la configuración de rutas múltiples

Verifique que el estado de multivía de NVMe en kernel, el estado de ANA y los espacios de nombres de ONTAP sean correctos para la configuración de NVMe-oF.

.Pasos
. Compruebe que la multivía NVMe en kernel esté habilitada:
+
[source, cli]
----
cat /sys/module/nvme_core/parameters/multipath
----
+
Debe ver la siguiente salida:

+
[listing]
----
Y
----
. Verifique que las configuraciones NVMe-oF apropiadas (por ejemplo, el modelo establecido en NetApp ONTAP Controller y la política de equilibrio de carga establecida en round-robin) para los respectivos espacios de nombres ONTAP se muestren correctamente en el host:
+
.. Mostrar los subsistemas:
+
[source, cli]
----
cat /sys/class/nvme-subsystem/nvme-subsys*/model
----
+
Debe ver la siguiente salida:

+
[listing]
----
NetApp ONTAP Controller
NetApp ONTAP Controller
----
.. Mostrar la política:
+
[source, cli]
----
cat /sys/class/nvme-subsystem/nvme-subsys*/iopolicy
----
+
Debe ver la siguiente salida:

+
[listing]
----
queue-depth
queue-depth
----


. Verifique que los espacios de nombres se hayan creado y detectado correctamente en el host:
+
[source, cli]
----
nvme list
----
+
.Muestra el ejemplo
[%collapsible]
====
[listing]
----
Node                               Generic             SN                   Model
--------------------------------------------------------------------------------------
/dev/nvme100n1  /dev/ng100n1  81LJCJYaKOHhAAAAAAAf   NetApp ONTAP Controller
Namespace Usage    Format             FW             Rev
-----------------------------------------------------------
0x1                 1.19  GB /   5.37  GB   4 KiB + 0 B   9.18.1
----
====
. Compruebe que el estado de la controladora de cada ruta sea activo y que tenga el estado de ANA correcto:
+
[role="tabbed-block"]
====
.NVMe/FC
--
[source, cli]
----
nvme list-subsys /dev/nvme100n1
----
.Muestra el ejemplo
[%collapsible]
=====
[listing, subs="+quotes"]
----
nvme-subsys4 - NQN=nqn.1992-08.com.netapp:sn.3623e199617311f09257d039eaa7dfc9:subsystem.Nvme31
               hostnqn=nqn.2014-08.org.nvmexpress:uuid: 4c4c4544-0056-5410-8048-b9c04f42563
               \
+- nvme199 *fc*   traddr=nn-0x200fd039eaa7dfc8:pn-0x2010d039eaa7dfc8,host_traddr=nn-0x200000109bf044b1:pn-0x100000109bf044b1 *live optimized*
+- nvme246 *fc*  traddr=nn-0x200fd039eaa7dfc8:pn-0x2011d039eaa7dfc8,host_traddr=nn-0x200000109bf044b1:pn-0x100000109bf044b1  *live non-optimized*
+- nvme249 *fc*  traddr=nn-0x200fd039eaa7dfc8:pn-0x2013d039eaa7dfc8,host_traddr=nn-0x200000109bf044b2:pn-0x100000109bf044b2 *live optimized*
+- nvme251 *fc*   traddr=nn-0x200fd039eaa7dfc8:pn-0x2012d039eaa7dfc8,host_traddr=nn-0x200000109bf044b2:pn-0x100000109bf044b2 *live non-optimized*
----
=====
--
.NVMe/TCP
--
[source, cli]
----
nvme list-subsys /dev/nvme0n1
----
.Muestra el ejemplo
[%collapsible]
=====
[listing, subs="+quotes"]
----
nvme-subsys0 - NQN=nqn.1992-08.com.netapp:sn.51a3c9846e0c11f08f5dd039eaa7dfc9:subsystem.Nvme1
hostnqn=nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0035-5910-804b-b5c04f444d33
\
+- nvme0 *tcp* traddr=192.168.30.48,trsvcid=4420,host_traddr=192.168.30.15,
src_addr=192.168.30.15 *live optimized*
+- nvme1 *tcp* traddr=192.168.30.49,trsvcid=4420,host_traddr=192.168.30.15,
src_addr=192.168.30.15 *live non-optimized*
+- nvme2 *tcp* traddr=192.168.31.48,trsvcid=4420,host_traddr=192.168.31.15,
src_addr=192.168.31.15 *live optimized*
+- nvme3 *tcp* traddr=192.168.31.49,trsvcid=4420,host_traddr=192.168.31.15,
src_addr=192.168.31.15 *live non-optimized*
----
=====
--
====
. Confirmar que el complemento de NetApp muestra los valores correctos para cada dispositivo de espacio de nombres ONTAP:
+
[role="tabbed-block"]
====
.Columna
--
[source, cli]
----
nvme netapp ontapdevices -o column
----
.Muestra el ejemplo
[%collapsible]
=====
[listing, subs="+quotes"]
----
Device          Vserver         Subsystem   Namespace Path      NSID
------------    --------        ----------- -----------------   ----
/dev/nvme0n1    vs_iscsi_tcp    Nvme1       /vol/Nvmevol1/ns1   1
UUID                                    Size
-------------------------------------   -----
d8efef7d-4dde-447f-b50e-b2c009298c66    26.84GB

----
=====
--
.JSON
--
[source, cli]
----
nvme netapp ontapdevices -o json
----
.Muestra el ejemplo
[%collapsible]
=====
[listing, subs="+quotes"]
----
{
  "ONTAPdevices":[
    {
      "Device":"/dev/nvme0n1",
      "Vserver":"vs_iscsi_tcp",
      " Subsystem":"Nvme1",
      "Namespace_Path":"/vol/Nvmevol1/ns1",
      "NSID":1,
      "UUID":"d8efef7d-4dde-447f-b50e-b2c009298c66",
      "LBA_Size":4096,
      "Namespace_Size":26843545600,
    },
]
}
----
=====
--
====




== Paso 8: Configurar la autenticación segura en banda

La autenticación segura en banda es compatible con NVMe/TCP entre un host RHEL 9.x y un controlador ONTAP .

Cada host o controlador debe estar asociado con un  `DH-HMAC-CHAP` Clave para configurar la autenticación segura. A  `DH-HMAC-CHAP` La clave es una combinación del NQN del host o controlador NVMe y un secreto de autenticación configurado por el administrador. Para autenticar su par, un host o una controladora NVMe deben reconocer la clave asociada con el par.

Configure la autenticación segura en banda mediante la CLI o un archivo JSON de configuración. Si necesita especificar diferentes claves dhchap para diferentes subsistemas, debe utilizar un archivo JSON de configuración.

[role="tabbed-block"]
====
.CLI
--
Configure la autenticación segura en banda mediante la CLI.

. Obtenga el NQN del host:
+
[source, cli]
----
cat /etc/nvme/hostnqn
----
. Genere la clave dhchap para el host RHEL 9.x.
+
La siguiente salida describe el `gen-dhchap-key` parámetros del comando:

+
[listing]
----
nvme gen-dhchap-key -s optional_secret -l key_length {32|48|64} -m HMAC_function {0|1|2|3} -n host_nqn
•	-s secret key in hexadecimal characters to be used to initialize the host key
•	-l length of the resulting key in bytes
•	-m HMAC function to use for key transformation
0 = none, 1- SHA-256, 2 = SHA-384, 3=SHA-512
•	-n host NQN to use for key transformation
----
+
En el siguiente ejemplo, se genera una clave dhchap aleatoria con HMAC establecido en 3 (SHA-512).

+
[listing]
----
nvme gen-dhchap-key -m 3 -n nqn.2014-
08.org.nvmexpress:uuid:e6dade64-216d-11ec-b7bb-7ed30a5482c3
DHHC-1:03:wSpuuKbBHTzC0W9JZxMBsYd9JFV8Si9aDh22k2BR/4m852vH7KGlrJeMpzhmyjDWOo0PJJM6yZsTeEpGkDHMHQ255+g=:
----
. En la controladora ONTAP, añada el host y especifique ambas claves dhchap:
+
[source, cli]
----
vserver nvme subsystem host add -vserver <svm_name> -subsystem <subsystem> -host-nqn <host_nqn> -dhchap-host-secret <authentication_host_secret> -dhchap-controller-secret <authentication_controller_secret> -dhchap-hash-function {sha-256|sha-512} -dhchap-group {none|2048-bit|3072-bit|4096-bit|6144-bit|8192-bit}
----
. Un host admite dos tipos de métodos de autenticación: Unidireccional y bidireccional. En el host, conéctese a la controladora ONTAP y especifique claves dhchap según el método de autenticación elegido:
+
[source, cli]
----
nvme connect -t tcp -w <host-traddr> -a <tr-addr> -n <host_nqn> -S <authentication_host_secret> -C <authentication_controller_secret>
----
. Valide el `nvme connect authentication` comando mediante la verificación de las claves dhchap de host y controladora:
+
.. Verifique las claves dhchap del host:
+
[source, cli]
----
cat /sys/class/nvme-subsystem/<nvme-subsysX>/nvme*/dhchap_secret
----
+
.Mostrar ejemplo de salida para una configuración unidireccional
[%collapsible]
=====
[listing]
----
cat /sys/class/nvme-subsystem/nvme-subsys1/nvme*/dhchap_secret
DHHC-1:01:hhdIYK7rGxHiNYS4d421GxHeDRUAuY0vmdqCp/NOaYND2PSc:
DHHC-1:01:hhdIYK7rGxHiNYS4d421GxHeDRUAuY0vmdqCp/NOaYND2PSc:
DHHC-1:01:hhdIYK7rGxHiNYS4d421GxHeDRUAuY0vmdqCp/NOaYND2PSc:
DHHC-1:01:hhdIYK7rGxHiNYS4d421GxHeDRUAuY0vmdqCp/NOaYND2PSc:
----
=====
.. Compruebe las claves dhchap del controlador:
+
[source, cli]
----
cat /sys/class/nvme-subsystem/<nvme-subsysX>/nvme*/dhchap_ctrl_secret
----
+
.Mostrar ejemplo de salida para una configuración bidireccional
[%collapsible]
=====
[listing]
----
cat /sys/class/nvme-subsystem/nvme-
subsys*/nvme*/dhchap_ctrl_secret

DHHC-1:03:ZCRrP9MQOeXhFitT7Fvvf/3P6K/qY1HfSmSfM8nLjESJdOjbjK/J6m00ygJgjm0VrRlrgrnHzjtWJmsnoVBO3rPDGEk=:
DHHC-1:03:ZCRrP9MQOeXhFitT7Fvvf/3P6K/qY1HfSmSfM8nLjESJdOjbjK/J6m00ygJgjm0VrRlrgrnHzjtWJmsnoVBO3rPDGEk=:
DHHC-1:03:ZCRrP9MQOeXhFitT7Fvvf/3P6K/qY1HfSmSfM8nLjESJdOjbjK/J6m00ygJgjm0VrRlrgrnHzjtWJmsnoVBO3rPDGEk=:
DHHC-1:03:ZCRrP9MQOeXhFitT7Fvvf/3P6K/qY1HfSmSfM8nLjESJdOjbjK/J6m00ygJgjm0VrRlrgrnHzjtWJmsnoVBO3rPDGEk=:
----
=====




--
.JSON
--
Cuando hay varios subsistemas NVMe disponibles en el controlador ONTAP , puede utilizar el `/etc/nvme/config.json` archivo con el `nvme connect-all` dominio.

Utilice el `-o` Opción para generar el archivo JSON.  Consulte las páginas del manual de NVMe connect-all para obtener más opciones de sintaxis.

. Configurar el archivo JSON.
+

NOTE: En el siguiente ejemplo,  `dhchap_key` corresponde a  `dhchap_secret` y  `dhchap_ctrl_key` corresponde a  `dhchap_ctrl_secret` .

+
.Muestra el ejemplo
[%collapsible]
=====
[listing]
----
cat /etc/nvme/config.json
[
{
  "hostnqn":"nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0035-5910-804b-b5c04f444d33",
  "hostid":"4c4c4544-0035-5910-804b-b5c04f444d33",
  "dhchap_key":"DHHC-1:01:GhgaLS+0h0W/IxKhSa0iaMHg17SOHRTzBduPzoJ6LKEJs3/f:",
  "subsystems":[
        {
          "nqn":"nqn.1992-08.com.netapp:sn.2c0c80d9873a11f0bc60d039eab6cb6d:subsystem.istpMNTC_subsys",
          "ports":[
              {
                  "transport":"tcp",
                    "traddr":"192.168.30.44",
                  "host_traddr":"192.168.30.15",
                  "trsvcid":"4420",
                  "dhchap_ctrl_key":"DHHC-1:03:GaraCO84o/uM0jF4rKJlgTy22bVoV0dRn1M+9QDfQRNVwJDHfPu2LrK5Y+/XG8iGcRtBCdm3
fYm3ZmO6NiepCORoY5Q=:"
              },
              {
                  "transport":"tcp",
                  "traddr":"192.168.30.45"
                  "host_traddr":"192.168.30.15",
                  "trsvcid":"4420",
                  "dhchap_ctrl_key":"DHHC-1:03:GaraCO84o/uM0jF4rKJlgTy22bVoV0dRn1M+9QDfQRNVwJDHfPu2LrK5Y+/XG8iGcRtBCdm3
fYm3ZmO6NiepCORoY5Q=:"
              },
              {
                  "transport":"tcp",
                 "traddr":"192.168.31.44",
                  "host_traddr":"192.168.31.15",
                  "trsvcid":"4420",
                  "dhchap_ctrl_key":"DHHC-
                  1:03:
GaraCO84o/uM0jF4rKJlgTy22bVoV0dRn1M+9QDfQRNVwJDHfPu2LrK5Y+/XG8iGc
RtBCdm3fYm3ZmO6NiepCORoY5Q=:"                               },
 {
                  "transport":"tcp",
                  "traddr":"192.168.31.45",
                  "host_traddr":"192.168.31.15",
                  "trsvcid":"4420",
                  "dhchap_ctrl_key":"DHHC-
                  1:03:                                    GaraCO84o/uM0jF4rKJlgTy22bVoV0dRn1M+9QDfQRNVwJDHfPu2LrK5Y+/XG8iGcRtBCdm3fYm3ZmO6NiepCORoY5Q=:"
              }
        ]
  ]
}
]
----
=====
. Conéctese a la controladora ONTAP mediante el archivo JSON de configuración:
+
[source, cli]
----
nvme connect-all -J /etc/nvme/config.json
----
+
.Muestra el ejemplo
[%collapsible]
=====
[listing]
----
already connected to hostnqn=nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0035-5910-804b-b5c04f444d33,nqn=nqn.1992-08.com.netapp:sn.2c0c80d9873a11f0bc60d039eab6cb6d:subsystem.istpMNTC_subsys,transport=tcp,traddr=192.168.30.44,trsvcid=4420
already connected to hostnqn=nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0035-5910-804b-b5c04f444d33,nqn=nqn.1992-08.com.netapp:sn.2c0c80d9873a11f0bc60d039eab6cb6d:subsystem.istpMNTC_subsys,transport=tcp,traddr=192.168.31.44,trsvcid=4420
already connected to hostnqn=nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0035-5910-804b-b5c04f444d33,nqn=nqn.1992-08.com.netapp:sn.2c0c80d9873a11f0bc60d039eab6cb6d:subsystem.istpMNTC_subsys,transport=tcp,traddr=192.168.30.45,trsvcid=4420
already connected to hostnqn=nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0035-5910-804b-b5c04f444d33,nqn=nqn.1992-08.com.netapp:sn.2c0c80d9873a11f0bc60d039eab6cb6d:subsystem.istpMNTC_subsys,transport=tcp,traddr=192.168.31.45,trsvcid=4420
----
=====
. Verifique que se hayan activado los secretos dhchap para las respectivas controladoras de cada subsistema:
+
.. Verifique las claves dhchap del host:
+
[source, cli]
----
cat /sys/class/nvme-subsystem/nvme-subsys96/nvme96/dhchap_secret
----
+
El siguiente ejemplo muestra una clave dhchap:

+
[listing]
----
DHHC-1:01:hhdIYK7rGxHiNYS4d421GxHeDRUAuY0vmdqCp/NOaYND2PSc:
----
.. Compruebe las claves dhchap del controlador:
+
[source, cli]
----
cat /sys/class/nvme-subsystem/nvme-subsys96/nvme96/dhchap_ctrl_secret
----
+
Debería ver un resultado similar al siguiente ejemplo:

+
[listing]
----
DHHC-1:03:ZCRrP9MQOeXhFitT7Fvvf/3P6K/qY1HfSmSfM8nLjESJdOjbjK/J6m00ygJgjm0VrRlrgrnHzjtWJmsnoVBO3rPDGEk=:
----




--
====


== Paso 9: Revise los problemas conocidos

Estos son los problemas conocidos:

[cols="20,40,40"]
|===
| ID de error de NetApp | Título | Descripción 


| 1503468 | En RHEL 9.1, el `nvme list-subsys` El comando devuelve una lista repetida de controladores NVME para un subsistema determinado | El `nvme list-subsys` El comando devuelve una lista de controladores NVMe para un subsistema determinado.  En RHEL 9.1, este comando muestra los controladores con su estado ANA para todos los espacios de nombres del subsistema.  Dado que el estado de ANA es un atributo por espacio de nombres, el comando debe mostrar entradas de controlador únicas con el estado de la ruta para cada espacio de nombres. 


| link:https://mysupport.netapp.com/site/bugs-online/product/HOSTUTILITIES/BURT/1479047["1479047"^] | Los hosts NVMe-oF de RHEL 9.0 crean controladores de descubrimiento persistente (PDC) duplicados | En hosts NVMe-oF, puede utilizar el comando  `nvme discover -p` para crear CDP. Cuando se utiliza este comando, solo se debe crear un PDC por combinación iniciador-destino. Sin embargo, si está ejecutando Oracle Linux 8x con un host NVMe-oF, se crea un PDC duplicado cada vez que se ejecuta  `nvme discover -p`. Esto genera un uso innecesario de recursos tanto en el host como en el destino. 
|===